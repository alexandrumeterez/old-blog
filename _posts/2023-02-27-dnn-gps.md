---
layout: distill
title: Deep Neural Networks as Gaussian Processes 
description: notes on Lee et. al
giscus_comments: false
date: 2023-02-27

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Basics
  - name: Single layer network
  - name: Multiple layer network
  - name: Efficiently computing the GP kernel
  - name: Relationship to deep signal propagation
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
---

## Basics
- the paper derives the equivalence between **infinitely wide, deep networks** and GPs
- so far, the equivalence was established only for infinitely wide, 1 layer networks
- main idea is the CLT 

## Single layer network
Consider a **a single hidden layer** neural network, where $$ W_{ij}^l \sim \mathcal{N}(0, \sigma_w^2/{N_{L-1}}) $$ and $$ b_{i}^l \sim \mathcal{N}(0, \sigma_b^2) $$.

$$
\begin{align}
z_i^1(x) &= b_i^1 + \sum_{j=1}^{N_1}W_{ij}^1 x_j^1(x) \\
x_j^1(x) &= \phi(b_j^0 + \sum_{k=1}^{d_in}W_{jk}^0 x_k)
\end{align}
$$

where the superscript indictates the layer and the subscript indexes in the vector/matrix.

Notice that $$ z_i^l $$ is a linear combination of normally distributed terms. Taking $$ N_1 \to \infty $$ and applying CLT, we get that $$ z_i^1(x) \sim \mathcal{N}(\mu^1, \Sigma^1) $$ (note that $$ z_i^1(x) $$ is defined for only one sample point $$ x $$). Since $$ z_i^1(x) $$ is Gaussian $$ \forall x $$ in the dataset, we can conclude that the joint distribution over all samples in the dataset is a GP: 


$$ 
z_i^1 = (z_i^1(x^\alpha))_{\alpha=1\dots n} \sim \mathcal{GP}(\mu^1, K^1)
$$

where

$$
\begin{align}
K^1(x, x') &= \mathbb{E}[z_i^1(x) z_i^1(x')] \\
&= \mathbb{E}[(b_i^1)^2 + \sum_{j=1}^{N_1} W_{ij}^1 x_j^1(x) \sum_{j=1}^{N_1} W_{ij}^1 x_j^1(x')] \\
&= \sigma_b^2 + \sigma_w^2 \mathbb{E}[x_i^1(x) x_i^1(x')] \\
&= \sigma_b^2 + \sigma_w^2 C(x, x') 
\end{align}
$$


## Multiple layer network
The above result generalizes to multiple layer networks. If we take the widith of each layer to infinity $$ N_i \to \infty $$ in succession, by the same argument as before we get that $$ z_i^l(x) \sim \mathcal{N}(0, \Sigma^l) $$ and $$ z_i^l \sim \mathcal{GP}(0, K^l) $$, where:

$$
\begin{align}
K^l(x, x') &= \mathbb{E}[z_i^l(x) z_i^l(x')] \\
&= \sigma_b^2 + \sigma_w^2 \mathbb{E}_{z_i^{l-1}}[\phi(z_i^{l-1}(x)) \phi(z_i^{l-1}(x'))] \\
&= \sigma_b^2 + \sigma_w^2 F_\phi(K^{l-1}(x, x'), K^{l-1}(x, x), K^{l-1}(x', x'))
\end{align}
$$

Note that the computation is a recurrence relationship which depends on $$ K^{l-1} $$ via the deterministic function $$ F_\phi $$, which in turn depends on the nonlinearity. For some nonlinearities, the function can be computed analytically. 

## Efficiently computing the GP kernel
I will not reproduce it here, but in the paper the authors propose an efficient algorithm to compute the GP kernel, which reduces the complexity. See section 2.5 in the main paper.

## Relationship to deep signal propagation
There are several nice insights in this section, tying it to the signal propagation mean-field papers.

It is well known that as the depth $$ l \to \infty $$, the kernel converges to a constant fixed point $$ K^\infty(x, x') $$. However, looking at this fixed point is interesting and has been studied. 

Plotting for TanH, $$ K^\infty(x, x') $$ as a function of $$ \sigma_w^2, \sigma_b^2 $$, we observe 2 regions being formed:
- ordered phase (bias dominates): for different inputs, the features become similar; reason is that all inputs approach the bias vector $$ \implies K^l(x, x') \to q^* \forall x, x' $$, where $$ q^* = q^*(\sigma_w^2, \sigma_b^2) $$
- chaotic phase (weight dominates): for similar inputs, features become different;  $$ \implies K^l(x, x) \to q^* $$ but $$ K^l(x, x') \to q^* c^* $$

We aim for initialization on the boundary of these 2 regions.

Plotting for ReLU, $$ K^\infty(x, x') $$ as a function of $$ \sigma_w^2, \sigma_b^2 $$, we get $$ K^\infty(x, x') = q^* \forall x, x' $$ we also observe 2 regions being formed:
- bounded phase: $$ q^* $$ is finite
- unbounded phase: $$ q^* $$ is unbounded or infinite

In NNGP a similar phenomenon happens. Data flows through the model as long as $$ K^L(x, x') - K^\infty(x, x') \neq 0 $$. This difference shrinks as $$ L \to \infty $$.