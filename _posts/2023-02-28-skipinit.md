---
layout: distill
title: Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks 
description: notes on skipinit
giscus_comments: false
date: 2023-02-28

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Introduction

# if a section has subsections, you can add them as follows:
# subsections:
#   - name: Example Child Subsection 1
#   - name: Example Child Subsection 2
---

## Introduction

Before going in depth in this paper, I will note that it is basically the same idea as ReZero: scaling the residual branch at initialization to 0, thus obtaining identity - except they call it SkipInit.

Excerpt from the paper:
"SkipInit: Include a learnable scalar multiplier at the end of each residual branch".

Main insights: 
- Batch norm enables us to train deeper residual networks because it doenscales the residual branch $f^l(x^l)$ by $\frac{1}{\sqrt{l}}$.
- Batch normalized networks do not benefit from large learning rates when the batch size is small.
- Batch normalization has a regularizing effect.

An interesting discussion on SkipInit vs orthogonal initialization in TanH nets. Orthogonal initialization does well in TanH networks because TanH is approximately identity around the origin, hence the orthogonal weights only rotate/reflect the input. However, the same initialization does not work with ReLU because ReLU is not identity around the origin.

---

## Why are deep normalized resnets trainable?

### Setting and notation

We denote by $x_i^l$, the i-th input to the layer l, $x_i^+ = max(x_i, 0)$ and $\mathcal{B}(\cdot)$ is batch normalization. We assume $W^l_{ij} \sim \mathcal{N}(0, 2/\text{fan-in})$. We denote the input to the model $x^1$ and we know that $x^1_i \sim \mathcal{N}(0, 1)$. Also, we have that $x^{l+1} = x^l + f^l(x^l)$. We define a normalized network as $f^l(x^l) = W^l \mathcal{B}(x^{l+})$ and an unnormalized network as $f^l(x^l) = W^l x^{l+}$

We can easily see the following identities: 

$$ 
\begin{align} 
	\mathbb{E}[f_i^l(x^l) | x^l] &= 0 \\
	\mathbb{E}[x^{l+1}] &= \mathbb{E}[x^l] + \mathbb{E}[f^l(x^l)] \\
	&= \mathbb{E}[x^l] + \mathbb{E} [\mathbb{E}[f^l(x^l) | x^l]] \\
	&= \mathbb{E}[x^l] = \dots = \mathbb{E}[x^1] \\
	&= 0,  \forall l \\
\end{align} 
$$ 

Moreover, we have that:

$$ 
\begin{align} 
	Cov(f_i^l(x^l), x^l_i) &= \mathbb{E}[ Cov(f_i^l(x^l), x^l_i | x^l_i)] + Cov(\mathbb{E}[f_i^l(x^l) | x^l_i], \mathbb{E}[x^l_i | x^l_i]) \\
	&= 0 + Cov(0, x_i^l) \\
	&= 0 \\
	Var(x_i^{l+1}) &= Var(x_i^l) + Var(f_i^l(x^l))
\end{align} 
$$ 

Before moving forward, assuming $x=max(0, y)$, we write one basic identity regarding ReLU (can be derived by writing down the definition of expectation):

$$ 
\begin{align} 
\mathbb{E}[x^2] = \frac{1}{2}Var(y)
\end{align} 
$$ 


Based on the above statements, we can conclude the following:

#### Unnormalized networks

For the unnormalized type of networks:

$$ 
\begin{align} 
	Var(f_i^l(x^l)) &= Var(x_i^l) \\
	\implies Var(x_i^{l+1}) &= 2 Var(x_i^l) = 2^l & \text{Eq. 9} \\
	\implies Var(x_i^{l}) &= 2^{l-1} + 2^l
\end{align} 
$$ 

which means that the **variance in depth explodes** and that the residuals and the skip connection contribute equally to the output. In order to control for this explosion, we can scale $x^{l+1}$ by a $\frac{1}{\sqrt{2}}$ factor.

#### Normalized networks

For normalized networks:
$$ 
\begin{align} 
	Var(f_i^l(x^l)) &= Var(\mathcal{B}(x^l)_i) \approx 1 & \text{large batch} \\
	\implies Var(x_i^{l+1}) &\approx Var(x_i^l) + 1 \approx l & \text{Eq. 9} \\
	\implies Var(x_i^{l}) &=  (l-1) + 1
\end{align} 
$$ 

which means that **batch norm prevents exponential variance explosion in depth** and that the contribution of the skip connection is $l$ times larger than the residual. In conclusion, the output of the skip connection dominates the residuals, biasing the network outputs towards identity. This means that batch norm **actually scales $x^{l+1}$ by $\frac{1}{\sqrt{l}}$**.

It is important to note that both the forward pass and the backward pass are properly scaled by the batch normalization operation.