<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="side-note">Side note</h2> <p>Papers marked with ✅, are the ones I’ve read in depth (and preferably took notes on).</p> <p>There is no particular order to these sections.</p> <hr> <h2 id="training-without-normalization">Training without normalization</h2> <ul> <li>✅ Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks</li> <li>Fixup Initialization: Residual Learning Without Normalization</li> </ul> <hr> <h2 id="ntk">NTK</h2> <ul> <li>✅ Neural Tangent Kernel: Convergence and Generalization in Neural Networks</li> <li>✅ Deep Neural Networks as Gaussian Processes</li> <li>✅ Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent</li> <li>Tensor Programs II: Neural Tangent Kernel for Any Architecture</li> </ul> <hr> <h2 id="mean-field">Mean Field</h2> <ul> <li>Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice</li> <li>Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks</li> <li>Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks</li> <li>Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs</li> <li>The Emergence of Spectral Universality in Deep Networks</li> <li>Mean Field Residual Networks: On the Edge of Chaos</li> <li>Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.</li> </ul> <hr> <h2 id="transformers">Transformers</h2> <ul> <li>Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping</li> <li>Deep Learning without Shortcuts: Shaping the Kernel with Tailored Rectifiers</li> <li>Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation</li> <li>[https://transformer-circuits.pub/2021/framework/index.html]</li> <li>[https://transformer-circuits.pub/2021/framework/index.html]</li> <li>Infinite attention: NNGP and NTK for deep attention networks</li> <li>[https://hyunjik11.github.io/talks/Attention_the_Analogue_of_Kernels_in_Deep_Learning.pdf]</li> </ul> <hr> <h3 id="rank-collapse">Rank collapse</h3> <ul> <li>Attention is not all you need: pure attention loses rank doubly exponentially with depth</li> <li>Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse</li> <li>✅ ReZero is All You Need: Fast Convergence at Large Depth</li> </ul> </body></html>