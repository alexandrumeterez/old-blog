<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="introduction">Introduction</h2> <p>Before going in depth in this paper, I will note that it is basically the same idea as ReZero: scaling the residual branch at initialization to 0, thus obtaining identity - except they call it SkipInit.</p> <p>Excerpt from the paper: “SkipInit: Include a learnable scalar multiplier at the end of each residual branch”.</p> <p>Main insights:</p> <ul> <li>Batch norm enables us to train deeper residual networks because it doenscales the residual branch $f^l(x^l)$ by $\frac{1}{\sqrt{l}}$.</li> <li>Batch normalized networks do not benefit from large learning rates when the batch size is small.</li> <li>Batch normalization has a regularizing effect.</li> </ul> <p>An interesting discussion on SkipInit vs orthogonal initialization in TanH nets. Orthogonal initialization does well in TanH networks because TanH is approximately identity around the origin, hence the orthogonal weights only rotate/reflect the input. However, the same initialization does not work with ReLU because ReLU is not identity around the origin.</p> <hr> <h2 id="why-are-deep-normalized-resnets-trainable">Why are deep normalized resnets trainable?</h2> <h3 id="setting-and-notation">Setting and notation</h3> <p>We denote by $x_i^l$, the i-th input to the layer l, $x_i^+ = max(x_i, 0)$ and $\mathcal{B}(\cdot)$ is batch normalization. We assume $W^l_{ij} \sim \mathcal{N}(0, 2/\text{fan-in})$. We denote the input to the model $x^1$ and we know that $x^1_i \sim \mathcal{N}(0, 1)$. Also, we have that $x^{l+1} = x^l + f^l(x^l)$. We define a normalized network as $f^l(x^l) = W^l \mathcal{B}(x^{l+})$ and an unnormalized network as $f^l(x^l) = W^l x^{l+}$</p> <p>We can easily see the following identities:</p> \[\begin{align} \mathbb{E}[f_i^l(x^l) | x^l] &amp;= 0 \\ \mathbb{E}[x^{l+1}] &amp;= \mathbb{E}[x^l] + \mathbb{E}[f^l(x^l)] \\ &amp;= \mathbb{E}[x^l] + \mathbb{E} [\mathbb{E}[f^l(x^l) | x^l]] \\ &amp;= \mathbb{E}[x^l] = \dots = \mathbb{E}[x^1] \\ &amp;= 0, \forall l \\ \end{align}\] <p>Moreover, we have that:</p> \[\begin{align} Cov(f_i^l(x^l), x^l_i) &amp;= \mathbb{E}[ Cov(f_i^l(x^l), x^l_i | x^l_i)] + Cov(\mathbb{E}[f_i^l(x^l) | x^l_i], \mathbb{E}[x^l_i | x^l_i]) \\ &amp;= 0 + Cov(0, x_i^l) \\ &amp;= 0 \\ Var(x_i^{l+1}) &amp;= Var(x_i^l) + Var(f_i^l(x^l)) \end{align}\] <p>Before moving forward, assuming $x=max(0, y)$, we write one basic identity regarding ReLU (can be derived by writing down the definition of expectation):</p> \[\begin{align} \mathbb{E}[x^2] = \frac{1}{2}Var(y) \end{align}\] <p>Based on the above statements, we can conclude the following:</p> <h4 id="unnormalized-networks">Unnormalized networks</h4> <p>For the unnormalized type of networks:</p> \[\begin{align} Var(f_i^l(x^l)) &amp;= Var(x_i^l) \\ \implies Var(x_i^{l+1}) &amp;= 2 Var(x_i^l) = 2^l &amp; \text{Eq. 9} \\ \implies Var(x_i^{l}) &amp;= 2^{l-1} + 2^l \end{align}\] <p>which means that the <strong>variance in depth explodes</strong> and that the residuals and the skip connection contribute equally to the output. In order to control for this explosion, we can scale $x^{l+1}$ by a $\frac{1}{\sqrt{2}}$ factor.</p> <h4 id="normalized-networks">Normalized networks</h4> <p>For normalized networks: \(\begin{align} Var(f_i^l(x^l)) &amp;= Var(\mathcal{B}(x^l)_i) \approx 1 &amp; \text{large batch} \\ \implies Var(x_i^{l+1}) &amp;\approx Var(x_i^l) + 1 \approx l &amp; \text{Eq. 9} \\ \implies Var(x_i^{l}) &amp;= (l-1) + 1 \end{align}\)</p> <p>which means that <strong>batch norm prevents exponential variance explosion in depth</strong> and that the contribution of the skip connection is $l$ times larger than the residual. In conclusion, the output of the skip connection dominates the residuals, biasing the network outputs towards identity. This means that batch norm <strong>actually scales $x^{l+1}$ by $\frac{1}{\sqrt{l}}$</strong>.</p> <p>It is important to note that both the forward pass and the backward pass are properly scaled by the batch normalization operation.</p> </body></html>