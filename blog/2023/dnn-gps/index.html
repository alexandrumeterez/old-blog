<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Deep Neural Networks as Gaussian Processes | Alexandru Meterez</title> <meta name="author" content="Alexandru Meterez"> <meta name="description" content="notes on Lee et. al"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/alexandrumeterez.github.io/assets/css/main.css"> <link rel="canonical" href="https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/dnn-gps/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/alexandrumeterez.github.io/assets/js/theme.js"></script> <script src="/alexandrumeterez.github.io/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"]]}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/alexandrumeterez.github.io/assets/js/distillpub/template.v2.js"></script> <script src="/alexandrumeterez.github.io/assets/js/distillpub/transforms.v2.js"></script> <script src="/alexandrumeterez.github.io/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Deep Neural Networks as Gaussian Processes",
      "description": "notes on Lee et. al",
      "published": "February 27, 2023",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="alexandrumeterez.github.io/"><span class="font-weight-bold">Alexandru </span>Meterez</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/alexandrumeterez.github.io/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/alexandrumeterez.github.io/blog/">notes<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Deep Neural Networks as Gaussian Processes</h1> <p>notes on Lee et. al</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#basics">Basics</a></div> <div><a href="#single-layer-network">Single layer network</a></div> <div><a href="#multiple-layer-network">Multiple layer network</a></div> <div><a href="#efficiently-computing-the-gp-kernel">Efficiently computing the GP kernel</a></div> <div><a href="#relationship-to-deep-signal-propagation">Relationship to deep signal propagation</a></div> </nav> </d-contents> <h2 id="basics">Basics</h2> <ul> <li>the paper derives the equivalence between <strong>infinitely wide, deep networks</strong> and GPs</li> <li>so far, the equivalence was established only for infinitely wide, 1 layer networks</li> <li>main idea is the CLT</li> </ul> <h2 id="single-layer-network">Single layer network</h2> <p>Consider a <strong>a single hidden layer</strong> neural network, where \(W_{ij}^l \sim \mathcal{N}(0, \sigma_w^2/{N_{L-1}})\) and \(b_{i}^l \sim \mathcal{N}(0, \sigma_b^2)\).</p> \[\begin{align} z_i^1(x) &amp;= b_i^1 + \sum_{j=1}^{N_1}W_{ij}^1 x_j^1(x) \\ x_j^1(x) &amp;= \phi(b_j^0 + \sum_{k=1}^{d_in}W_{jk}^0 x_k) \end{align}\] <p>where the superscript indictates the layer and the subscript indexes in the vector/matrix.</p> <p>Notice that \(z_i^l\) is a linear combination of normally distributed terms. Taking \(N_1 \to \infty\) and applying CLT, we get that \(z_i^1(x) \sim \mathcal{N}(\mu^1, \Sigma^1)\) (note that \(z_i^1(x)\) is defined for only one sample point \(x\)). Since \(z_i^1(x)\) is Gaussian \(\forall x\) in the dataset, we can conclude that the joint distribution over all samples in the dataset is a GP:</p> \[z_i^1 = (z_i^1(x^\alpha))_{\alpha=1\dots n} \sim \mathcal{GP}(\mu^1, K^1)\] <p>where</p> \[\begin{align} K^1(x, x') &amp;= \mathbb{E}[z_i^1(x) z_i^1(x')] \\ &amp;= \mathbb{E}[(b_i^1)^2 + \sum_{j=1}^{N_1} W_{ij}^1 x_j^1(x) \sum_{j=1}^{N_1} W_{ij}^1 x_j^1(x')] \\ &amp;= \sigma_b^2 + \sigma_w^2 \mathbb{E}[x_i^1(x) x_i^1(x')] \\ &amp;= \sigma_b^2 + \sigma_w^2 C(x, x') \end{align}\] <h2 id="multiple-layer-network">Multiple layer network</h2> <p>The above result generalizes to multiple layer networks. If we take the widith of each layer to infinity \(N_i \to \infty\) in succession, by the same argument as before we get that \(z_i^l(x) \sim \mathcal{N}(0, \Sigma^l)\) and \(z_i^l \sim \mathcal{GP}(0, K^l)\), where:</p> \[\begin{align} K^l(x, x') &amp;= \mathbb{E}[z_i^l(x) z_i^l(x')] \\ &amp;= \sigma_b^2 + \sigma_w^2 \mathbb{E}_{z_i^{l-1}}[\phi(z_i^{l-1}(x)) \phi(z_i^{l-1}(x'))] \\ &amp;= \sigma_b^2 + \sigma_w^2 F_\phi(K^{l-1}(x, x'), K^{l-1}(x, x), K^{l-1}(x', x')) \end{align}\] <p>Note that the computation is a recurrence relationship which depends on \(K^{l-1}\) via the deterministic function \(F_\phi\), which in turn depends on the nonlinearity. For some nonlinearities, the function can be computed analytically.</p> <h2 id="efficiently-computing-the-gp-kernel">Efficiently computing the GP kernel</h2> <p>I will not reproduce it here, but in the paper the authors propose an efficient algorithm to compute the GP kernel, which reduces the complexity. See section 2.5 in the main paper.</p> <h2 id="relationship-to-deep-signal-propagation">Relationship to deep signal propagation</h2> <p>There are several nice insights in this section, tying it to the signal propagation mean-field papers.</p> <p>It is well known that as the depth \(l \to \infty\), the kernel converges to a constant fixed point \(K^\infty(x, x')\). However, looking at this fixed point is interesting and has been studied.</p> <p>Plotting for TanH, \(K^\infty(x, x')\) as a function of \(\sigma_w^2, \sigma_b^2\), we observe 2 regions being formed:</p> <ul> <li>ordered phase (bias dominates): for different inputs, the features become similar; reason is that all inputs approach the bias vector \(\implies K^l(x, x') \to q^* \forall x, x'\), where \(q^* = q^*(\sigma_w^2, \sigma_b^2)\)</li> <li>chaotic phase (weight dominates): for similar inputs, features become different; \(\implies K^l(x, x) \to q^*\) but \(K^l(x, x') \to q^* c^*\)</li> </ul> <p>We aim for initialization on the boundary of these 2 regions.</p> <p>Plotting for ReLU, \(K^\infty(x, x')\) as a function of \(\sigma_w^2, \sigma_b^2\), we get \(K^\infty(x, x') = q^* \forall x, x'\) we also observe 2 regions being formed:</p> <ul> <li>bounded phase: \(q^*\) is finite</li> <li>unbounded phase: \(q^*\) is unbounded or infinite</li> </ul> <p>In NNGP a similar phenomenon happens. Data flows through the model as long as \(K^L(x, x') - K^\infty(x, x') \neq 0\). This difference shrinks as \(L \to \infty\).</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/alexandrumeterez.github.io/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Alexandru Meterez. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>