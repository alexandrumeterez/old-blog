<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Neural Tangent Kernel | Alexandru Meterez</title> <meta name="author" content="Alexandru Meterez"> <meta name="description" content="notes on NTK and related material"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/alexandrumeterez.github.io/assets/css/main.css"> <link rel="canonical" href="https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/neural-tangent-kernel/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/alexandrumeterez.github.io/assets/js/theme.js"></script> <script src="/alexandrumeterez.github.io/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"]]}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/alexandrumeterez.github.io/assets/js/distillpub/template.v2.js"></script> <script src="/alexandrumeterez.github.io/assets/js/distillpub/transforms.v2.js"></script> <script src="/alexandrumeterez.github.io/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Neural Tangent Kernel",
      "description": "notes on NTK and related material",
      "published": "February 26, 2023",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="alexandrumeterez.github.io/"><span class="font-weight-bold">AlexandruÂ </span>Meterez</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/alexandrumeterez.github.io/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/alexandrumeterez.github.io/blog/">notes<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Neural Tangent Kernel</h1> <p>notes on NTK and related material</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#functional-gradient-descent">Functional Gradient Descent</a></div> <div><a href="#functionals">Functionals</a></div> <ul> <li><a href="#reproducing-kernel-hilbert-space-rkhs">Reproducing Kernel Hilbert Space (RKHS)</a></li> <li><a href="#inner-product-and-norm">Inner product and norm</a></li> <li><a href="#reproducing-property">Reproducing property</a></li> <li><a href="#evaluation-functional">Evaluation functional</a></li> <li><a href="#functional-derivative">Functional derivative</a></li> <li><a href="#chain-rule">Chain rule</a></li> </ul> <div><a href="#gaussian-processes-gp">Gaussian Processes (GP)</a></div> <div><a href="#neural-tangent-kernel">Neural Tangent Kernel</a></div> <ul> <li><a href="#setting">Setting</a></li> <li><a href="#random-functions-approximation">Random functions approximation</a></li> <li><a href="#ntk">NTK</a></li> </ul> </nav> </d-contents> <h2 id="functional-gradient-descent">Functional Gradient Descent</h2> <p>For examples and plots see <a href="https://simple-complexities.github.io/optimization/functional/gradient/descent/2020/03/04/functional-gradient-descent.html" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>Suppose we want to learn a function \(f(x)\) using gradient descent. One example would be to parameterize \(f\) as a linear function using weights \(w\): \(f(x) = w^\top x\).</p> <p>In order to learn \(w\) we take a loss function, i.e. MSE:</p> <p>\(\begin{align} L(w) &amp;= \sum_{i=1}^n (y_i - w^\top x_i)^2 + \| w \|^2 \\ \nabla L(w) &amp;= \sum_{i=1}^n -2 x_i (y_i - w^\top x_i) + 2w \\ &amp;= \sum_{i=1}^n (-2x_iy_i + 2w^\top \| x_i \|^2) + 2w \end{align}\).</p> <p>Then we do gradient descent steps on $w$:</p> \[w = w - \alpha \nabla L(w)\] <p>Using functional gradient descent, this can be generalized to any function \(f\):</p> \[\begin{align} L(f) &amp;= \sum_{i=1}^n (y_i - f(x_i)) + \| f \|^2 \\ f_{t+1} &amp;= f_t - \alpha \nabla L(f) \end{align}\] <p>This has 2 advantages:</p> <ul> <li>Some loss functions are non-convex in parameter space but can be convex in functional space</li> <li>In NTK, when width \(\rightarrow \infty\), the weights become almost static between GD steps; however we can still study the function trajectory during GD in functional space</li> </ul> <hr> <h2 id="functionals">Functionals</h2> <p>Some basic functional notions are needed to understand this post.</p> <h3 id="reproducing-kernel-hilbert-space-rkhs">Reproducing Kernel Hilbert Space (RKHS)</h3> <p>Denote RKHS of (fixed) kernel \(k\) by \(\mathcal{H_k}\), \(k(\cdot, \cdot)\) is a kernel function and \(K_{ij} = k(x_i, x_j)\). Then:</p> \[f \in \mathcal{H_k} \implies f(\cdot) = \sum_{i=1}^n \beta_i k(x_i, \cdot), \beta_i \in \mathbb{R}\] <p>In other words, \(f\) is in RKHS if it can be written as a weighted sum of kernel functions evaluated over \(n\) points. Note that \(f\) is completely determined by the \(\beta_i\) and \(x_i\).</p> <h3 id="inner-product-and-norm">Inner product and norm</h3> <p>Let \(f, g \in \mathcal{H_k}\). Then:</p> \[\begin{align} f \cdot g &amp;= \sum_{i=1}^{n_f} \sum_{j=1}^{n_g} \alpha_i \beta_j k(x_i, x_j) = \alpha K^\top \beta \\ \| f \|^2 &amp;= f \cdot f = \alpha K^\top \beta \end{align}\] <h3 id="reproducing-property">Reproducing property</h3> \[f \cdot k(x, \cdot) = \sum_{i=1}^n \beta_i [k(x_i, \cdot) k(x, \cdot)] = \sum_{i=1}^n \beta_i k(x_i, x) = f\] <h3 id="evaluation-functional">Evaluation functional</h3> \[E_x[f] = f(x)\] <h3 id="functional-derivative">Functional derivative</h3> <p>From the definition of the derivative, the functional derivative is the coefficient of the linear term in the Taylor expansion of the functional.</p> <p>Example 1 - for the functional \(E_x[f] = f(x)\):</p> \[\begin{align} E_x[f + df] &amp;= f(x) + df(x) \\ &amp;= E_x[f] + df(x) \\ &amp;= E_x[f] + k(x, \cdot) \cdot df \\ &amp;\implies \nabla E_x[f] = k(x, \cdot) \end{align}\] <p>Example 2 - for the functional \(E[f] = \| f \|^2\)</p> \[\begin{align} E[f + df] &amp;= (f + df) (f + df) \\ &amp;= \| f \| + 2 f \cdot df + \| df \|^2 \\ &amp;= E[f] + 2 f \cdot df + \| df \|^2 \\ &amp;\implies \nabla E[f] = 2f \end{align}\] <h3 id="chain-rule">Chain rule</h3> <p>Let \(E[f]\) be a functional and \(g : \mathbb{R} \to \mathbb{R}\). Then:</p> \[\nabla g(E[f]) = \nabla E[f] g'(E[f])\] <h2 id="gaussian-processes-gp">Gaussian Processes (GP)</h2> <p>This is a (very) short and handwavy recap on GPs, but it suffices to show the relevant connections to NTK.</p> <p>Let \(Y \sim \mathcal{N(0, \Sigma_y)}\) be the training set and \(X \sim \mathcal{N(0, \Sigma_x)}\) be the test set (we can always substract the mean to center them in 0). We setup the prior distribution \(P_X\) such that \(\Sigma_x^{ij} = K(X_i, X_j)\). We also have the joint distribution</p> \[P_{X, Y} = \begin{bmatrix} X \\ Y \end{bmatrix} \sim \mathcal{N}(0, \begin{bmatrix} \Sigma_{xx} &amp; \Sigma_{xy} \\ \Sigma_{yx} &amp; \Sigma_{yy}\end{bmatrix})\] <p>where each covariance is setup similarly via the kernel \(K\). Then, due to the property that the Normal distribution is closed under conditioning and marginalization, we can compute the posterior</p> \[P_{X|Y}\] <p>From the posterior we can sample multiple functions that fit our training data.</p> <h2 id="neural-tangent-kernel">Neural Tangent Kernel</h2> <ul> <li>We know that at initialization, when taking width \(\to \infty\), ANNs behave like GPs</li> <li>NTK proves that this is also true during training, where the GP kernel used in the training is the âNeural Tangent Kernelâ</li> <li>The NTK stays constant during training when taking width \(\to \infty\)</li> </ul> <p>In the following derivations, I will <strong>not use biases</strong>.</p> <h3 id="setting">Setting</h3> <p>The following setting is used in the paper:</p> <p>\(\begin{align} f_\theta(x) &amp;= \tilde{\alpha}^{(L)}(x) \\ \alpha^{(0)}(x) &amp;= x \\ \tilde{\alpha}^{(l+1)}(x) &amp;= \frac{1}{\sqrt{n_l}} W^{(l)} \alpha^{(l)}(x) \\ \alpha^{(l)} &amp;= \sigma(\tilde{\alpha}^{(l)}(x)) \end{align}\) where \(W_{ij}^{(l)} \sim \mathcal{N}(0, \frac{1}{n_l})\).</p> <p>Define the bilinear form:</p> \[\langle f, g \rangle_{p^{in}} = \mathbb{E}_{x \sim p^{in}}[ f(x)^\top g(x)]\] <p>where \(p^{in}\) is the distribution of the input set (assume empirical distribution over \(N\) points). Similarly,</p> \[\langle f, g \rangle_{K} = \mathbb{E}_{x, x' \sim p^{in}}[ f(x)^\top K(x, x') g(x')]\] <p>.</p> <p>Also, let \(F^{(L)}: \mathbb{R}^P \to \mathcal{F}\) be the realization function, which maps parameters \(\theta\) to a function \(f_\theta\) (basically takes in parameters and returns a function parameterized by these parameters), and \(\nabla_{W_{ij}^{(l)}}F^{(L)}\) be the derivative of the realization function w.r.t. the weights. Define also \(\mu : \mathcal{F} \to \mathbb{R}, \mu = \langle d, \cdot \rangle_{p^{in}}, d \in \mathcal{F}\). Plugging in \(d = K_{i, \cdot}(x, \cdot)\) in the previous definition (since \(K_{i, \cdot}(x, \cdot) \in \mathcal{F}\)), we get:</p> \[\begin{align} f_{\mu, i}(x) = \langle d, K_{i, \cdot}(x, \cdot) \rangle \end{align}\] <p>Instead of doing gradient descent on the parameters \(\theta\) (which we will see stay almost constant during training as the width \(\to \infty\)), we do functional gradient descent on the function \(f_\theta\) itself, using a cost \(C : \mathcal{F} \to \mathbb{R}\).</p> <p>Define the functional derivative of \(C\) at a point</p> <p>\(f_0 \in \mathcal{F}\) as \(\nabla_fC|_{f_0}\)</p> <p>and the dual</p> \[d|_{f_0} \in \mathcal{F}\] <p>such that</p> \[\nabla_fC|_{f_0} = \langle d|_{f_0}, \cdot \rangle_{p^{in}}\] <p>.</p> <hr> <h3 id="random-functions-approximation">Random functions approximation</h3> <p>Before moving onto ANNs, this is a simplification in which the realization function is linear.</p> <p>Let \(P\) random functions \(f^{(p)}\) define a random linear parametrization:</p> \[\begin{align} F^{lin} = f_\theta^{lin} = \frac{1}{\sqrt{P}} \sum_{p=1}^P \theta_p f^{(p)} \end{align}\] <p>The partial derivatives are then:</p> \[\begin{align} \nabla_{\theta_p} F^{lin}(\theta) &amp;= \frac{1}{\sqrt{P}} f^{(p)} \\ \nabla_{t} F^{lin}(\theta(t)) &amp;= \frac{1}{\sqrt{P}} \sum_{p=1}^P \nabla_t \theta_p(t) f^{(p)} \\ \end{align}\] <p>Writing down the gradient descent step on the parameters:</p> \[\begin{align} \theta_p(t+dt) &amp;= \theta_p(t) - dt\nabla_{\theta_p}(C \circ F^{lin})|_{\theta(t)} \\ \implies \nabla_t \theta_p(t) &amp;= -\nabla_{\theta_p}(C \circ F^{lin})|_{\theta(t)} \\ &amp;= -\nabla_{\theta_p}F^{lin} \nabla_{F^{lin}}C|_{f_{\theta(t)}^{lin}} \\ &amp;= -(\frac{1}{\sqrt{P}} f^{(p)}) \nabla_{F^{lin}}C|_{f_{\theta(t)}^{lin}} \\ &amp;= -\frac{1}{\sqrt{P}} \langle d|_{f_{\theta(t)}^{lin}}, f^{(p)} \rangle_{p^{in}} \end{align}\] <p>Plugging in the above 2 equations we get the evolution of the function \(f_{\theta(t)}^{lin}\) in function space through GD:</p> \[\begin{align} \nabla_{t} f^{lin}_{\theta(t)} &amp;= -\frac{1}{P} \sum_{p=1}^P \langle d|_{f_{\theta(t)}^{lin}}, f^{(p)} \rangle_{p^{in}} f^{(p)} \\ &amp;= - \nabla_{\tilde{K}}C|_{f_\theta^{lin}} \end{align}\] <p>where \(\tilde{K}\) is the tangent kernel (by the definition in the paper in section 3).</p> <p>In conclusion, parameter gradient descent on \(C \circ F^{lin}\) is equivalent to functional gradient descent in the function space with the tangent kernel \(\tilde{K}\).</p> <h3 id="ntk">NTK</h3> <p>Similar to above, in ANNs, the network function evolution is:</p> \[\begin{align} \nabla_t f_{\theta(t)} &amp;= - \nabla_{\Theta^{(L)}C|_{f_{\theta(t)}}} \\ \Theta^{L}(\theta) &amp;= \sum_{p=1}^P \nabla_{\theta_p}F^{(L)}(\theta) \otimes \nabla_{\theta_p}F^{(L)}(\theta) \end{align}\] <p>where \(\Theta^{L}(\theta)\) is the NTK.</p> <p>For the following statements, check proof in the paper.</p> <h4 id="at-initialization">At initialization</h4> <p>Recall that \(f_\theta(x)\) is the preactivation of the final layer in an \(L\) layer deep ANN and \(f_{\theta, k}(x), k=1,\dots,n_L\) are the neurons in the final layer preactivation. At initialization, when taking the width of each layer to infinity, i.e. \(n_1, \dots n_{L-1} \to \infty\), the neurons \(f_{\theta,k}\) converge to GPs, with covariance \(\Sigma_L\), defined recursively by:</p> \[\begin{align} \Sigma^{(1)}(x, x') &amp;= \frac{1}{n_0}x^\top x' \\ \Sigma^{(L+1)}(x, x') &amp;= \mathbb{E}_{f \sim \mathcal{N}(0, \Sigma^{(L)})}[\sigma(f(x))\sigma(f(x'))] \end{align}\] <p>Note that, the above result showed that <strong>each of the output neurons</strong> converges to a GP, each with its own covariance matrix. However, the stronger result showed in the paper is that under the same conditions and the same limit, <strong>all of the covariance matrices</strong> converge to a deterministic NTK kernel \(\Theta^{(L)}\) (see definition in the paper under Theorem 1).</p> <h4 id="during-training">During training</h4> <p>It is also true that the NTK stays asymptotically constant during training, under the infinite width regime. Note that, during the infinite width regime, while the individual variation of each weight entry is small, the total variation is large, allowing the lower layers to learn.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/alexandrumeterez.github.io/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2023 Alexandru Meterez. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>