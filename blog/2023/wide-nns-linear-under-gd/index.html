<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent | Alexandru Meterez</title> <meta name="author" content="Alexandru Meterez"> <meta name="description" content="notes on Lee et. al"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/alexandrumeterez.github.io/assets/css/main.css"> <link rel="canonical" href="https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/wide-nns-linear-under-gd/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/alexandrumeterez.github.io/assets/js/theme.js"></script> <script src="/alexandrumeterez.github.io/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"]]}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/alexandrumeterez.github.io/assets/js/distillpub/template.v2.js"></script> <script src="/alexandrumeterez.github.io/assets/js/distillpub/transforms.v2.js"></script> <script src="/alexandrumeterez.github.io/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent",
      "description": "notes on Lee et. al",
      "published": "February 27, 2023",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="alexandrumeterez.github.io/"><span class="font-weight-bold">Alexandru </span>Meterez</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/alexandrumeterez.github.io/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/alexandrumeterez.github.io/blog/">notes<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent</h1> <p>notes on Lee et. al</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#theoretical-results">Theoretical results</a></div> <ul> <li><a href="#setting">Setting</a></li> <li><a href="#linearized-networks">Linearized networks</a></li> <li><a href="#gps-from-gd-training">GPs from GD training</a></li> <li><a href="#infinite-width-networks-are-linearized-networks">Infinite width networks are linearized networks</a></li> </ul> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>In the previous posts, we saw that: 1) NTK - gradient descent is parameter space is equivalent to functional gradient descent in function space, following the Neural Tangent Kernel 2) Under the infinite width limit, both single layer and multilayer networks are equivalent to a GP with a kernel defined using a recursive formula</p> <p>In this paper, the authors show that for wide neural networks (i.e. infinite width), the models can be linearized into their first-order Taylor expansion, around the parameters $w_0$ at initialization. In addition, they show that gradient-based training with MSE produces test set predictions drawn from a GP with a particular kernel.</p> <p>Main insight: when width $\to \infty$, models can be replaced by their first-order Taylor expansion, around $\theta_0$ (the params at initialization). This approximation holds well under finite width too.</p> <p>More concretely, a deep neural network with infinite width can be simplified to its linearized model. Under MSE, the dynamics become an ODE that can be solved in close form.</p> <hr> <h2 id="theoretical-results">Theoretical results</h2> <h3 id="setting">Setting</h3> <p>Note the abuse of notation: $\nabla_t f := \frac{df}{dt} = \dot{f}$. In the paper they use the differential instead of the derivative in writing the statements below (I divided by the learning rate $\eta$, where $\eta \to 0$).</p> <p>Setting is as in the previous posts, weights and biases are Gaussian distributed. The notation is:</p> \[\begin{align} h^{l+1} &amp;= x^l W^{l+1} + b^{l+1} \\ x^{l+1} &amp;= \phi(h^{l+1}) \end{align}\] <p>with layer width $n_l$. Also define $\theta^l = [W^l, b^l]$ the vector of all parameters for layer $l$ and $\theta$ the vector of all parameters in the network. Denote $f_t(x) = h^{L+1}(x) \in \mathbb{R}^k$, and $l(\hat{y}, y)$ as the loss function. Finally, $\mathcal{L} = \sum_{(x, y) \in \mathcal{D}} l(f_t(x), \hat{y})$.</p> <p>We can derive the ODE describing the evolution of parameters $\theta$ (also called gradient flow) as:</p> \[\begin{align} \theta_{t+1} &amp;= \theta_t - \eta \nabla_\theta \mathcal{L} \\ &amp;= \theta_t - \eta \nabla_\theta f_t(\mathcal{X})^\top \nabla_{f_t(\mathcal{X})}\mathcal{L} &amp; \text{chain rule}\\ \implies \frac{\theta_{t+1} - \theta_{t}}{\eta} &amp;= -\nabla_\theta f_t(\mathcal{X})^\top \nabla_{f_t(\mathcal{X})}\mathcal{L} \\ \implies \textcolor{red}{\dot{\theta_t}} &amp;= -\nabla_\theta f_t(\mathcal{X})^\top \nabla_{f_t(\mathcal{X})}\mathcal{L} &amp; \eta \to 0 \end{align}\] <p>Similarly, we can derive the evolution of the logits $f_t$:</p> \[\begin{align} \nabla_t f_t(\mathcal{X}) &amp;= \nabla_\theta f_t(\mathcal{X}) \textcolor{red}{\nabla_t \theta} &amp; \text{chain rule} \\ &amp;= -\underbrace{\nabla_\theta f_t(\mathcal{X})\nabla_\theta f_t(\mathcal{X})^\top}_{\Theta_t(\mathcal{X}, \mathcal{X})} \nabla_{f_t(\mathcal{X})}\mathcal{L} \\ \end{align}\] <p>where we refer to $\hat{\Theta}_t$ as the empirical tangent kernel.</p> <h3 id="linearized-networks">Linearized networks</h3> <p>We replace the outputs of the neural network with their Taylor expansion:</p> <p>\(\begin{align} f_t^{lin}(x) = f_0(x) + \nabla_\theta f_0(x)|_{\theta = \theta_0} \underbrace{(\theta_t - \theta_0)}_{\omega_t} \end{align}\) where $f_0$ is the initial output of the network.</p> <p>Plugging into the above derivation of gradient flow, we get:</p> \[\begin{align} \dot{\omega}_t &amp;= \nabla_\theta f_0(\mathcal{X})^\top \nabla_{f_t^{lin}(\mathcal{X})}\mathcal{L} \\ \dot{f_t}^{lin}(x) &amp;= \hat{\Theta}_0({x, \mathcal{X}}) \nabla_{f_t^{lin}(\mathcal{X})}\mathcal{L} \\ \end{align}\] <p>Under the MSE loss, these ODEs have clsoed form solutions (see section 2.2 in the paper). Therefore, without doing GD steps, we can compute the time evolution of the weights and the logits in a linearized neural network.</p> <p>Taking the width to infinity in each layer yields a Gaussian Process, with a certain mean and covariance described by a kernel (see form in paper, eq 13).</p> <h3 id="gps-from-gd-training">GPs from GD training</h3> <p>TODO (I don’t understand the details in this section too well) The main insight here is that $\forall x \in \mathcal{X}_{test}$, $f_t^{lin}(x)$ converges to a Gaussian distribution when taking the width $\to \infty$.</p> <h3 id="infinite-width-networks-are-linearized-networks">Infinite width networks are linearized networks</h3> <p>The authors show that applying GD with learning rate $\eta &lt; \eta_{critical}$ and taking the width of all layers $\to \infty$, then $f_t^{lin}(x) \to \mathcal{N}(\mu(\mathcal{X}_T), \Sigma(\mathcal{X}_T, \mathcal{X}_T))$ (see form of mean and covariance in paper, eq. 15).</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/alexandrumeterez.github.io/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Alexandru Meterez. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>