<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://alexandrumeterez.github.io/alexandrumeterez.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://alexandrumeterez.github.io/alexandrumeterez.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-02-28T16:10:12+00:00</updated><id>https://alexandrumeterez.github.io/alexandrumeterez.github.io/feed.xml</id><title type="html">blank</title><subtitle>Welcome to my little part of the world! </subtitle><entry><title type="html"></title><link href="https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/2023-02-25-reading-list/" rel="alternate" type="text/html" title=""/><published>2023-02-28T16:10:12+00:00</published><updated>2023-02-28T16:10:12+00:00</updated><id>https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/2023-02-25-reading-list</id><content type="html" xml:base="https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/2023-02-25-reading-list/"><![CDATA[<h2 id="side-note">Side note</h2> <p>Papers marked with ✅, are the ones I’ve read in depth (and preferably took notes on).</p> <p>There is no particular order to these sections.</p> <hr/> <h2 id="training-without-normalization">Training without normalization</h2> <ul> <li>✅ Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks</li> <li>Fixup Initialization: Residual Learning Without Normalization</li> </ul> <hr/> <h2 id="ntk">NTK</h2> <ul> <li>✅ Neural Tangent Kernel: Convergence and Generalization in Neural Networks</li> <li>✅ Deep Neural Networks as Gaussian Processes</li> <li>✅ Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent</li> <li>Tensor Programs II: Neural Tangent Kernel for Any Architecture</li> </ul> <hr/> <h2 id="mean-field">Mean Field</h2> <ul> <li>Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice</li> <li>Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks</li> <li>Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks</li> <li>Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs</li> <li>The Emergence of Spectral Universality in Deep Networks</li> <li>Mean Field Residual Networks: On the Edge of Chaos</li> <li>Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.</li> </ul> <hr/> <h2 id="transformers">Transformers</h2> <ul> <li>Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping</li> <li>Deep Learning without Shortcuts: Shaping the Kernel with Tailored Rectifiers</li> <li>Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation</li> <li>[https://transformer-circuits.pub/2021/framework/index.html]</li> <li>[https://transformer-circuits.pub/2021/framework/index.html]</li> <li>Infinite attention: NNGP and NTK for deep attention networks</li> <li>[https://hyunjik11.github.io/talks/Attention_the_Analogue_of_Kernels_in_Deep_Learning.pdf]</li> </ul> <hr/> <h3 id="rank-collapse">Rank collapse</h3> <ul> <li>Attention is not all you need: pure attention loses rank doubly exponentially with depth</li> <li>Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse</li> <li>✅ ReZero is All You Need: Fast Convergence at Large Depth</li> </ul>]]></content><author><name></name></author></entry><entry><title type="html"></title><link href="https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/2023-02-27-skipinit/" rel="alternate" type="text/html" title=""/><published>2023-02-28T16:10:12+00:00</published><updated>2023-02-28T16:10:12+00:00</updated><id>https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/2023-02-27-skipinit</id><content type="html" xml:base="https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/2023-02-27-skipinit/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Before going in depth in this paper, I will note that it is basically the same idea as ReZero: scaling the residual branch at initialization to 0, thus obtaining identity - except they call it SkipInit.</p> <p>Excerpt from the paper: “SkipInit: Include a learnable scalar multiplier at the end of each residual branch”.</p> <p>Main insights:</p> <ul> <li>Batch norm enables us to train deeper residual networks because it doenscales the residual branch $f^l(x^l)$ by $\frac{1}{\sqrt{l}}$.</li> <li>Batch normalized networks do not benefit from large learning rates when the batch size is small.</li> <li>Batch normalization has a regularizing effect.</li> </ul> <p>An interesting discussion on SkipInit vs orthogonal initialization in TanH nets. Orthogonal initialization does well in TanH networks because TanH is approximately identity around the origin, hence the orthogonal weights only rotate/reflect the input. However, the same initialization does not work with ReLU because ReLU is not identity around the origin.</p> <hr/> <h2 id="why-are-deep-normalized-resnets-trainable">Why are deep normalized resnets trainable?</h2> <h3 id="setting-and-notation">Setting and notation</h3> <p>We denote by $x_i^l$, the i-th input to the layer l, $x_i^+ = max(x_i, 0)$ and $\mathcal{B}(\cdot)$ is batch normalization. We assume $W^l_{ij} \sim \mathcal{N}(0, 2/\text{fan-in})$. We denote the input to the model $x^1$ and we know that $x^1_i \sim \mathcal{N}(0, 1)$. Also, we have that $x^{l+1} = x^l + f^l(x^l)$. We define a normalized network as $f^l(x^l) = W^l \mathcal{B}(x^{l+})$ and an unnormalized network as $f^l(x^l) = W^l x^{l+}$</p> <p>We can easily see the following identities:</p> \[\begin{align} \mathbb{E}[f_i^l(x^l) | x^l] &amp;= 0 \\ \mathbb{E}[x^{l+1}] &amp;= \mathbb{E}[x^l] + \mathbb{E}[f^l(x^l)] \\ &amp;= \mathbb{E}[x^l] + \mathbb{E} [\mathbb{E}[f^l(x^l) | x^l]] \\ &amp;= \mathbb{E}[x^l] = \dots = \mathbb{E}[x^1] \\ &amp;= 0, \forall l \\ \end{align}\] <p>Moreover, we have that:</p> \[\begin{align} Cov(f_i^l(x^l), x^l_i) &amp;= \mathbb{E}[ Cov(f_i^l(x^l), x^l_i | x^l_i)] + Cov(\mathbb{E}[f_i^l(x^l) | x^l_i], \mathbb{E}[x^l_i | x^l_i]) \\ &amp;= 0 + Cov(0, x_i^l) \\ &amp;= 0 \\ Var(x_i^{l+1}) &amp;= Var(x_i^l) + Var(f_i^l(x^l)) \end{align}\] <p>Before moving forward, assuming $x=max(0, y)$, we write one basic identity regarding ReLU (can be derived by writing down the definition of expectation):</p> \[\begin{align} \mathbb{E}[x^2] = \frac{1}{2}Var(y) \end{align}\] <p>Based on the above statements, we can conclude the following:</p> <h4 id="unnormalized-networks">Unnormalized networks</h4> <p>For the unnormalized type of networks:</p> \[\begin{align} Var(f_i^l(x^l)) &amp;= Var(x_i^l) \\ \implies Var(x_i^{l+1}) &amp;= 2 Var(x_i^l) = 2^l &amp; \text{Eq. 9} \\ \implies Var(x_i^{l}) &amp;= 2^{l-1} + 2^l \end{align}\] <p>which means that the <strong>variance in depth explodes</strong> and that the residuals and the skip connection contribute equally to the output. In order to control for this explosion, we can scale $x^{l+1}$ by a $\frac{1}{\sqrt{2}}$ factor.</p> <h4 id="normalized-networks">Normalized networks</h4> <p>For normalized networks: \(\begin{align} Var(f_i^l(x^l)) &amp;= Var(\mathcal{B}(x^l)_i) \approx 1 &amp; \text{large batch} \\ \implies Var(x_i^{l+1}) &amp;\approx Var(x_i^l) + 1 \approx l &amp; \text{Eq. 9} \\ \implies Var(x_i^{l}) &amp;= (l-1) + 1 \end{align}\)</p> <p>which means that <strong>batch norm prevents exponential variance explosion in depth</strong> and that the contribution of the skip connection is $l$ times larger than the residual. In conclusion, the output of the skip connection dominates the residuals, biasing the network outputs towards identity. This means that batch norm <strong>actually scales $x^{l+1}$ by $\frac{1}{\sqrt{l}}$</strong>.</p> <p>It is important to note that both the forward pass and the backward pass are properly scaled by the batch normalization operation.</p>]]></content><author><name></name></author></entry><entry><title type="html">Deep Neural Networks as Gaussian Processes</title><link href="https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/dnn-gps/" rel="alternate" type="text/html" title="Deep Neural Networks as Gaussian Processes"/><published>2023-02-27T00:00:00+00:00</published><updated>2023-02-27T00:00:00+00:00</updated><id>https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/dnn-gps</id><content type="html" xml:base="https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/dnn-gps/"><![CDATA[<h2 id="basics">Basics</h2> <ul> <li>the paper derives the equivalence between <strong>infinitely wide, deep networks</strong> and GPs</li> <li>so far, the equivalence was established only for infinitely wide, 1 layer networks</li> <li>main idea is the CLT</li> </ul> <h2 id="single-layer-network">Single layer network</h2> <p>Consider a <strong>a single hidden layer</strong> neural network, where \(W_{ij}^l \sim \mathcal{N}(0, \sigma_w^2/{N_{L-1}})\) and \(b_{i}^l \sim \mathcal{N}(0, \sigma_b^2)\).</p> \[\begin{align} z_i^1(x) &amp;= b_i^1 + \sum_{j=1}^{N_1}W_{ij}^1 x_j^1(x) \\ x_j^1(x) &amp;= \phi(b_j^0 + \sum_{k=1}^{d_in}W_{jk}^0 x_k) \end{align}\] <p>where the superscript indictates the layer and the subscript indexes in the vector/matrix.</p> <p>Notice that \(z_i^l\) is a linear combination of normally distributed terms. Taking \(N_1 \to \infty\) and applying CLT, we get that \(z_i^1(x) \sim \mathcal{N}(\mu^1, \Sigma^1)\) (note that \(z_i^1(x)\) is defined for only one sample point \(x\)). Since \(z_i^1(x)\) is Gaussian \(\forall x\) in the dataset, we can conclude that the joint distribution over all samples in the dataset is a GP:</p> \[z_i^1 = (z_i^1(x^\alpha))_{\alpha=1\dots n} \sim \mathcal{GP}(\mu^1, K^1)\] <p>where</p> \[\begin{align} K^1(x, x') &amp;= \mathbb{E}[z_i^1(x) z_i^1(x')] \\ &amp;= \mathbb{E}[(b_i^1)^2 + \sum_{j=1}^{N_1} W_{ij}^1 x_j^1(x) \sum_{j=1}^{N_1} W_{ij}^1 x_j^1(x')] \\ &amp;= \sigma_b^2 + \sigma_w^2 \mathbb{E}[x_i^1(x) x_i^1(x')] \\ &amp;= \sigma_b^2 + \sigma_w^2 C(x, x') \end{align}\] <h2 id="multiple-layer-network">Multiple layer network</h2> <p>The above result generalizes to multiple layer networks. If we take the widith of each layer to infinity \(N_i \to \infty\) in succession, by the same argument as before we get that \(z_i^l(x) \sim \mathcal{N}(0, \Sigma^l)\) and \(z_i^l \sim \mathcal{GP}(0, K^l)\), where:</p> \[\begin{align} K^l(x, x') &amp;= \mathbb{E}[z_i^l(x) z_i^l(x')] \\ &amp;= \sigma_b^2 + \sigma_w^2 \mathbb{E}_{z_i^{l-1}}[\phi(z_i^{l-1}(x)) \phi(z_i^{l-1}(x'))] \\ &amp;= \sigma_b^2 + \sigma_w^2 F_\phi(K^{l-1}(x, x'), K^{l-1}(x, x), K^{l-1}(x', x')) \end{align}\] <p>Note that the computation is a recurrence relationship which depends on \(K^{l-1}\) via the deterministic function \(F_\phi\), which in turn depends on the nonlinearity. For some nonlinearities, the function can be computed analytically.</p> <h2 id="efficiently-computing-the-gp-kernel">Efficiently computing the GP kernel</h2> <p>I will not reproduce it here, but in the paper the authors propose an efficient algorithm to compute the GP kernel, which reduces the complexity. See section 2.5 in the main paper.</p> <h2 id="relationship-to-deep-signal-propagation">Relationship to deep signal propagation</h2> <p>There are several nice insights in this section, tying it to the signal propagation mean-field papers.</p> <p>It is well known that as the depth \(l \to \infty\), the kernel converges to a constant fixed point \(K^\infty(x, x')\). However, looking at this fixed point is interesting and has been studied.</p> <p>Plotting for TanH, \(K^\infty(x, x')\) as a function of \(\sigma_w^2, \sigma_b^2\), we observe 2 regions being formed:</p> <ul> <li>ordered phase (bias dominates): for different inputs, the features become similar; reason is that all inputs approach the bias vector \(\implies K^l(x, x') \to q^* \forall x, x'\), where \(q^* = q^*(\sigma_w^2, \sigma_b^2)\)</li> <li>chaotic phase (weight dominates): for similar inputs, features become different; \(\implies K^l(x, x) \to q^*\) but \(K^l(x, x') \to q^* c^*\)</li> </ul> <p>We aim for initialization on the boundary of these 2 regions.</p> <p>Plotting for ReLU, \(K^\infty(x, x')\) as a function of \(\sigma_w^2, \sigma_b^2\), we get \(K^\infty(x, x') = q^* \forall x, x'\) we also observe 2 regions being formed:</p> <ul> <li>bounded phase: \(q^*\) is finite</li> <li>unbounded phase: \(q^*\) is unbounded or infinite</li> </ul> <p>In NNGP a similar phenomenon happens. Data flows through the model as long as \(K^L(x, x') - K^\infty(x, x') \neq 0\). This difference shrinks as \(L \to \infty\).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[notes on Lee et. al]]></summary></entry><entry><title type="html">Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent</title><link href="https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/wide-nns-linear-under-gd/" rel="alternate" type="text/html" title="Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent"/><published>2023-02-27T00:00:00+00:00</published><updated>2023-02-27T00:00:00+00:00</updated><id>https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/wide-nns-linear-under-gd</id><content type="html" xml:base="https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/wide-nns-linear-under-gd/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In the previous posts, we saw that: 1) NTK - gradient descent is parameter space is equivalent to functional gradient descent in function space, following the Neural Tangent Kernel 2) Under the infinite width limit, both single layer and multilayer networks are equivalent to a GP with a kernel defined using a recursive formula</p> <p>In this paper, the authors show that for wide neural networks (i.e. infinite width), the models can be linearized into their first-order Taylor expansion, around the parameters $w_0$ at initialization. In addition, they show that gradient-based training with MSE produces test set predictions drawn from a GP with a particular kernel.</p> <p>Main insight: when width $\to \infty$, models can be replaced by their first-order Taylor expansion, around $\theta_0$ (the params at initialization). This approximation holds well under finite width too.</p> <p>More concretely, a deep neural network with infinite width can be simplified to its linearized model. Under MSE, the dynamics become an ODE that can be solved in close form.</p> <hr/> <h2 id="theoretical-results">Theoretical results</h2> <h3 id="setting">Setting</h3> <p>Note the abuse of notation: $\nabla_t f := \frac{df}{dt} = \dot{f}$. In the paper they use the differential instead of the derivative in writing the statements below (I divided by the learning rate $\eta$, where $\eta \to 0$).</p> <p>Setting is as in the previous posts, weights and biases are Gaussian distributed. The notation is:</p> \[\begin{align} h^{l+1} &amp;= x^l W^{l+1} + b^{l+1} \\ x^{l+1} &amp;= \phi(h^{l+1}) \end{align}\] <p>with layer width $n_l$. Also define $\theta^l = [W^l, b^l]$ the vector of all parameters for layer $l$ and $\theta$ the vector of all parameters in the network. Denote $f_t(x) = h^{L+1}(x) \in \mathbb{R}^k$, and $l(\hat{y}, y)$ as the loss function. Finally, $\mathcal{L} = \sum_{(x, y) \in \mathcal{D}} l(f_t(x), \hat{y})$.</p> <p>We can derive the ODE describing the evolution of parameters $\theta$ (also called gradient flow) as:</p> \[\begin{align} \theta_{t+1} &amp;= \theta_t - \eta \nabla_\theta \mathcal{L} \\ &amp;= \theta_t - \eta \nabla_\theta f_t(\mathcal{X})^\top \nabla_{f_t(\mathcal{X})}\mathcal{L} &amp; \text{chain rule}\\ \implies \frac{\theta_{t+1} - \theta_{t}}{\eta} &amp;= -\nabla_\theta f_t(\mathcal{X})^\top \nabla_{f_t(\mathcal{X})}\mathcal{L} \\ \implies \textcolor{red}{\dot{\theta_t}} &amp;= -\nabla_\theta f_t(\mathcal{X})^\top \nabla_{f_t(\mathcal{X})}\mathcal{L} &amp; \eta \to 0 \end{align}\] <p>Similarly, we can derive the evolution of the logits $f_t$:</p> \[\begin{align} \nabla_t f_t(\mathcal{X}) &amp;= \nabla_\theta f_t(\mathcal{X}) \textcolor{red}{\nabla_t \theta} &amp; \text{chain rule} \\ &amp;= -\underbrace{\nabla_\theta f_t(\mathcal{X})\nabla_\theta f_t(\mathcal{X})^\top}_{\Theta_t(\mathcal{X}, \mathcal{X})} \nabla_{f_t(\mathcal{X})}\mathcal{L} \\ \end{align}\] <p>where we refer to $\hat{\Theta}_t$ as the empirical tangent kernel.</p> <h3 id="linearized-networks">Linearized networks</h3> <p>We replace the outputs of the neural network with their Taylor expansion:</p> <p>\(\begin{align} f_t^{lin}(x) = f_0(x) + \nabla_\theta f_0(x)|_{\theta = \theta_0} \underbrace{(\theta_t - \theta_0)}_{\omega_t} \end{align}\) where $f_0$ is the initial output of the network.</p> <p>Plugging into the above derivation of gradient flow, we get:</p> \[\begin{align} \dot{\omega}_t &amp;= \nabla_\theta f_0(\mathcal{X})^\top \nabla_{f_t^{lin}(\mathcal{X})}\mathcal{L} \\ \dot{f_t}^{lin}(x) &amp;= \hat{\Theta}_0({x, \mathcal{X}}) \nabla_{f_t^{lin}(\mathcal{X})}\mathcal{L} \\ \end{align}\] <p>Under the MSE loss, these ODEs have clsoed form solutions (see section 2.2 in the paper). Therefore, without doing GD steps, we can compute the time evolution of the weights and the logits in a linearized neural network.</p> <p>Taking the width to infinity in each layer yields a Gaussian Process, with a certain mean and covariance described by a kernel (see form in paper, eq 13).</p> <h3 id="gps-from-gd-training">GPs from GD training</h3> <p>TODO (I don’t understand the details in this section too well) The main insight here is that $\forall x \in \mathcal{X}_{test}$, $f_t^{lin}(x)$ converges to a Gaussian distribution when taking the width $\to \infty$.</p> <h3 id="infinite-width-networks-are-linearized-networks">Infinite width networks are linearized networks</h3> <p>The authors show that applying GD with learning rate $\eta &lt; \eta_{critical}$ and taking the width of all layers $\to \infty$, then $f_t^{lin}(x) \to \mathcal{N}(\mu(\mathcal{X}_T), \Sigma(\mathcal{X}_T, \mathcal{X}_T))$ (see form of mean and covariance in paper, eq. 15).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[notes on Lee et. al]]></summary></entry><entry><title type="html">Neural Tangent Kernel</title><link href="https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/neural-tangent-kernel/" rel="alternate" type="text/html" title="Neural Tangent Kernel"/><published>2023-02-26T00:00:00+00:00</published><updated>2023-02-26T00:00:00+00:00</updated><id>https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/neural-tangent-kernel</id><content type="html" xml:base="https://alexandrumeterez.github.io/alexandrumeterez.github.io/blog/2023/neural-tangent-kernel/"><![CDATA[<h2 id="functional-gradient-descent">Functional Gradient Descent</h2> <p>For examples and plots see <a href="https://simple-complexities.github.io/optimization/functional/gradient/descent/2020/03/04/functional-gradient-descent.html">here</a>.</p> <p>Suppose we want to learn a function \(f(x)\) using gradient descent. One example would be to parameterize \(f\) as a linear function using weights \(w\): \(f(x) = w^\top x\).</p> <p>In order to learn \(w\) we take a loss function, i.e. MSE:</p> <p>\(\begin{align} L(w) &amp;= \sum_{i=1}^n (y_i - w^\top x_i)^2 + \| w \|^2 \\ \nabla L(w) &amp;= \sum_{i=1}^n -2 x_i (y_i - w^\top x_i) + 2w \\ &amp;= \sum_{i=1}^n (-2x_iy_i + 2w^\top \| x_i \|^2) + 2w \end{align}\).</p> <p>Then we do gradient descent steps on $w$:</p> \[w = w - \alpha \nabla L(w)\] <p>Using functional gradient descent, this can be generalized to any function \(f\):</p> \[\begin{align} L(f) &amp;= \sum_{i=1}^n (y_i - f(x_i)) + \| f \|^2 \\ f_{t+1} &amp;= f_t - \alpha \nabla L(f) \end{align}\] <p>This has 2 advantages:</p> <ul> <li>Some loss functions are non-convex in parameter space but can be convex in functional space</li> <li>In NTK, when width \(\rightarrow \infty\), the weights become almost static between GD steps; however we can still study the function trajectory during GD in functional space</li> </ul> <hr/> <h2 id="functionals">Functionals</h2> <p>Some basic functional notions are needed to understand this post.</p> <h3 id="reproducing-kernel-hilbert-space-rkhs">Reproducing Kernel Hilbert Space (RKHS)</h3> <p>Denote RKHS of (fixed) kernel \(k\) by \(\mathcal{H_k}\), \(k(\cdot, \cdot)\) is a kernel function and \(K_{ij} = k(x_i, x_j)\). Then:</p> \[f \in \mathcal{H_k} \implies f(\cdot) = \sum_{i=1}^n \beta_i k(x_i, \cdot), \beta_i \in \mathbb{R}\] <p>In other words, \(f\) is in RKHS if it can be written as a weighted sum of kernel functions evaluated over \(n\) points. Note that \(f\) is completely determined by the \(\beta_i\) and \(x_i\).</p> <h3 id="inner-product-and-norm">Inner product and norm</h3> <p>Let \(f, g \in \mathcal{H_k}\). Then:</p> \[\begin{align} f \cdot g &amp;= \sum_{i=1}^{n_f} \sum_{j=1}^{n_g} \alpha_i \beta_j k(x_i, x_j) = \alpha K^\top \beta \\ \| f \|^2 &amp;= f \cdot f = \alpha K^\top \beta \end{align}\] <h3 id="reproducing-property">Reproducing property</h3> \[f \cdot k(x, \cdot) = \sum_{i=1}^n \beta_i [k(x_i, \cdot) k(x, \cdot)] = \sum_{i=1}^n \beta_i k(x_i, x) = f\] <h3 id="evaluation-functional">Evaluation functional</h3> \[E_x[f] = f(x)\] <h3 id="functional-derivative">Functional derivative</h3> <p>From the definition of the derivative, the functional derivative is the coefficient of the linear term in the Taylor expansion of the functional.</p> <p>Example 1 - for the functional \(E_x[f] = f(x)\):</p> \[\begin{align} E_x[f + df] &amp;= f(x) + df(x) \\ &amp;= E_x[f] + df(x) \\ &amp;= E_x[f] + k(x, \cdot) \cdot df \\ &amp;\implies \nabla E_x[f] = k(x, \cdot) \end{align}\] <p>Example 2 - for the functional \(E[f] = \| f \|^2\)</p> \[\begin{align} E[f + df] &amp;= (f + df) (f + df) \\ &amp;= \| f \| + 2 f \cdot df + \| df \|^2 \\ &amp;= E[f] + 2 f \cdot df + \| df \|^2 \\ &amp;\implies \nabla E[f] = 2f \end{align}\] <h3 id="chain-rule">Chain rule</h3> <p>Let \(E[f]\) be a functional and \(g : \mathbb{R} \to \mathbb{R}\). Then:</p> \[\nabla g(E[f]) = \nabla E[f] g'(E[f])\] <h2 id="gaussian-processes-gp">Gaussian Processes (GP)</h2> <p>This is a (very) short and handwavy recap on GPs, but it suffices to show the relevant connections to NTK.</p> <p>Let \(Y \sim \mathcal{N(0, \Sigma_y)}\) be the training set and \(X \sim \mathcal{N(0, \Sigma_x)}\) be the test set (we can always substract the mean to center them in 0). We setup the prior distribution \(P_X\) such that \(\Sigma_x^{ij} = K(X_i, X_j)\). We also have the joint distribution</p> \[P_{X, Y} = \begin{bmatrix} X \\ Y \end{bmatrix} \sim \mathcal{N}(0, \begin{bmatrix} \Sigma_{xx} &amp; \Sigma_{xy} \\ \Sigma_{yx} &amp; \Sigma_{yy}\end{bmatrix})\] <p>where each covariance is setup similarly via the kernel \(K\). Then, due to the property that the Normal distribution is closed under conditioning and marginalization, we can compute the posterior</p> \[P_{X|Y}\] <p>From the posterior we can sample multiple functions that fit our training data.</p> <h2 id="neural-tangent-kernel">Neural Tangent Kernel</h2> <ul> <li>We know that at initialization, when taking width \(\to \infty\), ANNs behave like GPs</li> <li>NTK proves that this is also true during training, where the GP kernel used in the training is the “Neural Tangent Kernel”</li> <li>The NTK stays constant during training when taking width \(\to \infty\)</li> </ul> <p>In the following derivations, I will <strong>not use biases</strong>.</p> <h3 id="setting">Setting</h3> <p>The following setting is used in the paper:</p> <p>\(\begin{align} f_\theta(x) &amp;= \tilde{\alpha}^{(L)}(x) \\ \alpha^{(0)}(x) &amp;= x \\ \tilde{\alpha}^{(l+1)}(x) &amp;= \frac{1}{\sqrt{n_l}} W^{(l)} \alpha^{(l)}(x) \\ \alpha^{(l)} &amp;= \sigma(\tilde{\alpha}^{(l)}(x)) \end{align}\) where \(W_{ij}^{(l)} \sim \mathcal{N}(0, \frac{1}{n_l})\).</p> <p>Define the bilinear form:</p> \[\langle f, g \rangle_{p^{in}} = \mathbb{E}_{x \sim p^{in}}[ f(x)^\top g(x)]\] <p>where \(p^{in}\) is the distribution of the input set (assume empirical distribution over \(N\) points). Similarly,</p> \[\langle f, g \rangle_{K} = \mathbb{E}_{x, x' \sim p^{in}}[ f(x)^\top K(x, x') g(x')]\] <p>.</p> <p>Also, let \(F^{(L)}: \mathbb{R}^P \to \mathcal{F}\) be the realization function, which maps parameters \(\theta\) to a function \(f_\theta\) (basically takes in parameters and returns a function parameterized by these parameters), and \(\nabla_{W_{ij}^{(l)}}F^{(L)}\) be the derivative of the realization function w.r.t. the weights. Define also \(\mu : \mathcal{F} \to \mathbb{R}, \mu = \langle d, \cdot \rangle_{p^{in}}, d \in \mathcal{F}\). Plugging in \(d = K_{i, \cdot}(x, \cdot)\) in the previous definition (since \(K_{i, \cdot}(x, \cdot) \in \mathcal{F}\)), we get:</p> \[\begin{align} f_{\mu, i}(x) = \langle d, K_{i, \cdot}(x, \cdot) \rangle \end{align}\] <p>Instead of doing gradient descent on the parameters \(\theta\) (which we will see stay almost constant during training as the width \(\to \infty\)), we do functional gradient descent on the function \(f_\theta\) itself, using a cost \(C : \mathcal{F} \to \mathbb{R}\).</p> <p>Define the functional derivative of \(C\) at a point</p> <p>\(f_0 \in \mathcal{F}\) as \(\nabla_fC|_{f_0}\)</p> <p>and the dual</p> \[d|_{f_0} \in \mathcal{F}\] <p>such that</p> \[\nabla_fC|_{f_0} = \langle d|_{f_0}, \cdot \rangle_{p^{in}}\] <p>.</p> <hr/> <h3 id="random-functions-approximation">Random functions approximation</h3> <p>Before moving onto ANNs, this is a simplification in which the realization function is linear.</p> <p>Let \(P\) random functions \(f^{(p)}\) define a random linear parametrization:</p> \[\begin{align} F^{lin} = f_\theta^{lin} = \frac{1}{\sqrt{P}} \sum_{p=1}^P \theta_p f^{(p)} \end{align}\] <p>The partial derivatives are then:</p> \[\begin{align} \nabla_{\theta_p} F^{lin}(\theta) &amp;= \frac{1}{\sqrt{P}} f^{(p)} \\ \nabla_{t} F^{lin}(\theta(t)) &amp;= \frac{1}{\sqrt{P}} \sum_{p=1}^P \nabla_t \theta_p(t) f^{(p)} \\ \end{align}\] <p>Writing down the gradient descent step on the parameters:</p> \[\begin{align} \theta_p(t+dt) &amp;= \theta_p(t) - dt\nabla_{\theta_p}(C \circ F^{lin})|_{\theta(t)} \\ \implies \nabla_t \theta_p(t) &amp;= -\nabla_{\theta_p}(C \circ F^{lin})|_{\theta(t)} \\ &amp;= -\nabla_{\theta_p}F^{lin} \nabla_{F^{lin}}C|_{f_{\theta(t)}^{lin}} \\ &amp;= -(\frac{1}{\sqrt{P}} f^{(p)}) \nabla_{F^{lin}}C|_{f_{\theta(t)}^{lin}} \\ &amp;= -\frac{1}{\sqrt{P}} \langle d|_{f_{\theta(t)}^{lin}}, f^{(p)} \rangle_{p^{in}} \end{align}\] <p>Plugging in the above 2 equations we get the evolution of the function \(f_{\theta(t)}^{lin}\) in function space through GD:</p> \[\begin{align} \nabla_{t} f^{lin}_{\theta(t)} &amp;= -\frac{1}{P} \sum_{p=1}^P \langle d|_{f_{\theta(t)}^{lin}}, f^{(p)} \rangle_{p^{in}} f^{(p)} \\ &amp;= - \nabla_{\tilde{K}}C|_{f_\theta^{lin}} \end{align}\] <p>where \(\tilde{K}\) is the tangent kernel (by the definition in the paper in section 3).</p> <p>In conclusion, parameter gradient descent on \(C \circ F^{lin}\) is equivalent to functional gradient descent in the function space with the tangent kernel \(\tilde{K}\).</p> <h3 id="ntk">NTK</h3> <p>Similar to above, in ANNs, the network function evolution is:</p> \[\begin{align} \nabla_t f_{\theta(t)} &amp;= - \nabla_{\Theta^{(L)}C|_{f_{\theta(t)}}} \\ \Theta^{L}(\theta) &amp;= \sum_{p=1}^P \nabla_{\theta_p}F^{(L)}(\theta) \otimes \nabla_{\theta_p}F^{(L)}(\theta) \end{align}\] <p>where \(\Theta^{L}(\theta)\) is the NTK.</p> <p>For the following statements, check proof in the paper.</p> <h4 id="at-initialization">At initialization</h4> <p>Recall that \(f_\theta(x)\) is the preactivation of the final layer in an \(L\) layer deep ANN and \(f_{\theta, k}(x), k=1,\dots,n_L\) are the neurons in the final layer preactivation. At initialization, when taking the width of each layer to infinity, i.e. \(n_1, \dots n_{L-1} \to \infty\), the neurons \(f_{\theta,k}\) converge to GPs, with covariance \(\Sigma_L\), defined recursively by:</p> \[\begin{align} \Sigma^{(1)}(x, x') &amp;= \frac{1}{n_0}x^\top x' \\ \Sigma^{(L+1)}(x, x') &amp;= \mathbb{E}_{f \sim \mathcal{N}(0, \Sigma^{(L)})}[\sigma(f(x))\sigma(f(x'))] \end{align}\] <p>Note that, the above result showed that <strong>each of the output neurons</strong> converges to a GP, each with its own covariance matrix. However, the stronger result showed in the paper is that under the same conditions and the same limit, <strong>all of the covariance matrices</strong> converge to a deterministic NTK kernel \(\Theta^{(L)}\) (see definition in the paper under Theorem 1).</p> <h4 id="during-training">During training</h4> <p>It is also true that the NTK stays asymptotically constant during training, under the infinite width regime. Note that, during the infinite width regime, while the individual variation of each weight entry is small, the total variation is large, allowing the lower layers to learn.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[notes on NTK and related material]]></summary></entry></feed>